{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep, strftime\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Mark/Downloads/chromedriver_win32/chromedriver.exe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Me = \"Users/Mark/Downloads\"\n",
    "\n",
    "chromedriver_path = f'C:/{Me}/chromedriver_win32/chromedriver.exe'\n",
    "\n",
    "chromedriver_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=chromedriver_path)\n",
    "sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update job variable to accept a list\n",
    "\n",
    "job = 'Data Science'\n",
    "city = 'Denver, CO'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening link to hiring website\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"hide-fixed-top\"]/a').click()\n",
    "sleep(2)\n",
    "# maybe a popup will activate\n",
    "try:\n",
    "    driver.find_element_by_id(\"external-apply-no-thanks\").click()\n",
    "except Exception as e:\n",
    "    pass\n",
    "sleep(2)\n",
    "\n",
    "# links to both the recomendation page and the linked to company website\n",
    "\n",
    "links = []\n",
    "for handle in driver.window_handles:\n",
    "    driver.switch_to.window(handle)\n",
    "    links.append(driver.current_url)\n",
    "print(links[1])\n",
    "\n",
    "#  close the company site and return to search results\n",
    "\n",
    "driver.switch_to.window(driver.window_handles[1]);\n",
    "driver.close();\n",
    "driver.switch_to.window(driver.window_handles[0]);\n",
    "driver.get(career_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_search(job, city):\n",
    "    \"\"\"accepts job title and city/zipcode in string format\"\"\"\n",
    "    \n",
    "    career_builder = (f'https://www.careerbuilder.com/jobs?utf8=%E2%9C%93&keywords={job}&location={city}')\n",
    "    driver.get(career_builder)\n",
    "    sleep(randint(8,10))\n",
    "    \n",
    "# example div: <div class=\"data-results-title dark-blue-text b\">Associate Data Scientist</div>\n",
    "    leads = []\n",
    "    # would like to iterate for every div class=\"data-results-title dark-blue-text b\"\n",
    "    for lead in range(1,4):\n",
    "        lead = driver.find_element_by_xpath(f'//*[@id=\"jobs_collection\"]/div[{lead}]/a[2]/div/div[2]/div[1]')\n",
    "        leads.append(lead)\n",
    "\n",
    "#     leads[1].click() <-would like to iterate through every lead in leads using div class rather than range(1,4)\n",
    "    \n",
    "    listed = []\n",
    "#     for div in divs:\n",
    "#         \"this is how I'd like to call the page_scrape()\"\n",
    "#     set the URL for each lead and point to it before scraping\n",
    "    listed.append(page_scrape())\n",
    "    \n",
    "    # saving a new dataframe as an excel file. the name is custom made to your cities and dates\n",
    "\n",
    "    final_df = listed[0].append(listed[1]).append(listed[2])\n",
    "    final_df.to_excel('search_backups//{}_from_{}.xlsx'.format(strftime(\"%Y%m%d-%H%M\"),\n",
    "                                                            job_site), index=False)\n",
    "    print('saved df.....')\n",
    "    \n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Haven't ran this code yet\n",
    "    username = 'YOUREMAIL@hotmail.com'\n",
    "    password = 'YOUR PASSWORD'\n",
    "\n",
    "    server = smtplib.SMTP('smtp.outlook.com', 587)\n",
    "    server.ehlo()\n",
    "    server.starttls()\n",
    "    server.login(username, password)\n",
    "    msg = ('Subject: Flight Scraper\\n\\n\\\n",
    "Cheapest Flight: {}\\nAverage Price: {}\\n\\nRecommendation: {}\\n\\nEnd of message'.format(matrix_min, matrix_avg, (loading+'\\n'+prediction)))\n",
    "    message = MIMEMultipart()\n",
    "    message['From'] = 'YOUREMAIL@hotmail.com'\n",
    "    message['to'] = 'YOUROTHEREMAIL@domain.com'\n",
    "    server.sendmail('YOUREMAIL@hotmail.com', 'YOUROTHEREMAIL@domain.com', msg)\n",
    "    print('sent email.....')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking over a test call we see tangentally associated results.  SEO analyst, data analytics, QA, etc...\n",
    "we want links to pages which actually match with data scientist.\n",
    "\n",
    "here's an example:\n",
    "\n",
    "<div class=\"data-results-title dark-blue-text b\">Associate Data Scientist</div>\n",
    "\n",
    "so lets grab stuff that contains the string mapped to the 'job' variable using\n",
    "where contains job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-55b04cdb31a5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-55b04cdb31a5>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    where contains job\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# for i in list:\n",
    "#     do something where i contains string from job variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_scrape():\n",
    "    \"\"\"This function does the scraping \n",
    "    currently working from page to be scraped and with no varible passed to fx\"\"\"\n",
    "    \n",
    "\n",
    "#     sections_list = [page for pages in leads]\n",
    "\n",
    "    # if you run into a reCaptcha, you might want to do something about it\n",
    "    # you will know there's a problem if the lists above are empty\n",
    "    # this if statement lets you exit the bot or do something else\n",
    "    # you can add a sleep here, to let you solve the captcha and continue scraping\n",
    "    # i'm using a SystemExit because i want to test everything from the start\n",
    "#     if section_a_list == []:\n",
    "#         raise SystemExit\n",
    "    \n",
    "    #now scrape valid info\n",
    "    # options.  only get link to site. scrape text and link to site. scrape overview and link to site.  click out of popup for site link.\n",
    "    # also have links to the company social availible...\n",
    "\n",
    "    desc = driver.find_element_by_xpath('//*[@id=\"jdp_description\"]')\n",
    "\n",
    "    title = driver.find_element_by_xpath('//*[@id=\"jdp-data\"]/div[1]/div[2]/div/div[1]/h1').text\n",
    "    text = BeautifulSoup(desc.text, \"html.parser\")\n",
    "    text_str = str(text)\n",
    "    text_list = text_str.split(\"\\n\")\n",
    "    print(f\"{text_list[0:5]} ...\")\n",
    "    \n",
    "    # opening link to hiring website\n",
    "\n",
    "    driver.find_element_by_xpath('//*[@id=\"hide-fixed-top\"]/a').click()\n",
    "    sleep(2)\n",
    "    # maybe trigger a popup\n",
    "    try:\n",
    "        driver.find_element_by_id(\"external-apply-no-thanks\").click()\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    sleep(2)\n",
    "\n",
    "    # links to both the recomendation page and the linked to company website\n",
    "\n",
    "    links = []\n",
    "    for handle in driver.window_handles:\n",
    "        driver.switch_to.window(handle)\n",
    "        links.append(driver.current_url)\n",
    "    print(links[1])\n",
    "\n",
    "    #  close the company site and return to search results\n",
    "\n",
    "    driver.switch_to.window(driver.window_handles[1]);\n",
    "    driver.close();\n",
    "    driver.switch_to.window(driver.window_handles[0]);\n",
    "    driver.get(career_builder)\n",
    "    sleep(3)\n",
    "\n",
    "    \n",
    "    cols = (['Job Title', \"Summary\", \"Link to Company Site\", \"Link to Job Refrence\", \"Day Scraped\"])\n",
    "    vals = {'Job Title': title,\n",
    "           'Summary': text_list,\n",
    "           'Link to Comany Site': links[1],\n",
    "           'Link to Job Refrence': links[0],\n",
    "           'Day Scraped': current_time_pretty}\n",
    "    \n",
    "    jobs_df = pd.DataFrame([vals])\n",
    "\n",
    "    return jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust this cell to customize scrape frequency\n",
    "# \n",
    "\n",
    "job = input('What kind of job do you want?')\n",
    "city = input('And in which city? ')\n",
    "\n",
    "\n",
    "\n",
    "for n in range(0,5):\n",
    "    start_search(job=job, city=city)\n",
    "    print('iteration {} was complete @ {}'.format(n, strftime(\"%Y%m%d-%H%M\")))\n",
    "    \n",
    "    # Wait 12 hours\n",
    "    sleep(60*60*12)\n",
    "    print('sleep finished.....')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
